{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scIDiff Basic Usage Tutorial (Fixed Version)\n",
    "\n",
    "This notebook demonstrates the basic usage of scIDiff for single-cell gene expression modeling and inverse design.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "Before running this notebook, make sure to install the package in development mode:\n",
    "\n",
    "```bash\n",
    "# Navigate to the scIDiff directory\n",
    "cd /path/to/scIDiff\n",
    "\n",
    "# Install in development mode\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "Or if you're running from the repository directory, you can add the path to sys.path as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to Python path (if not installed as package)\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scIDiff components\n",
    "try:\n",
    "    # Try importing as installed package\n",
    "    from scIDiff.models import ScIDiffModel\n",
    "    from scIDiff.training import ScIDiffTrainer\n",
    "    from scIDiff.sampling import InverseDesigner, PhenotypeTarget, GeneExpressionObjective\n",
    "    from scIDiff.data import SingleCellDataset\n",
    "    print(\"Successfully imported scIDiff components!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    print(\"Please install the package using: pip install -e .\")\n",
    "    print(\"Or make sure you're running from the correct directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "First, let's create some synthetic single-cell data for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic single-cell data\n",
    "def create_synthetic_data(n_cells=1000, n_genes=2000, n_cell_types=5):\n",
    "    \"\"\"\n",
    "    Create synthetic single-cell RNA-seq data\n",
    "    \"\"\"\n",
    "    # Generate cell type labels\n",
    "    cell_types = np.random.randint(0, n_cell_types, n_cells)\n",
    "    \n",
    "    # Generate gene expression data with cell type specific patterns\n",
    "    expression_data = []\n",
    "    \n",
    "    for cell_type in range(n_cell_types):\n",
    "        n_cells_type = np.sum(cell_types == cell_type)\n",
    "        \n",
    "        # Create cell type specific expression pattern\n",
    "        base_expression = np.random.lognormal(0, 1, (n_cells_type, n_genes))\n",
    "        \n",
    "        # Add cell type specific markers\n",
    "        marker_genes = np.random.choice(n_genes, 50, replace=False)\n",
    "        base_expression[:, marker_genes] *= (cell_type + 1) * 2\n",
    "        \n",
    "        # Add sparsity (many genes are not expressed)\n",
    "        sparsity_mask = np.random.random((n_cells_type, n_genes)) < 0.7\n",
    "        base_expression[sparsity_mask] = 0\n",
    "        \n",
    "        expression_data.append(base_expression)\n",
    "    \n",
    "    # Combine all cell types\n",
    "    expression_matrix = np.vstack(expression_data)\n",
    "    \n",
    "    return expression_matrix, cell_types\n",
    "\n",
    "# Generate synthetic data\n",
    "expression_data, cell_type_labels = create_synthetic_data()\n",
    "\n",
    "print(f\"Expression data shape: {expression_data.shape}\")\n",
    "print(f\"Cell types: {np.unique(cell_type_labels)}\")\n",
    "print(f\"Sparsity: {(expression_data == 0).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Expression distribution\n",
    "axes[0].hist(expression_data.flatten(), bins=50, alpha=0.7)\n",
    "axes[0].set_xlabel('Expression')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Expression Distribution')\n",
    "\n",
    "# Cell type distribution\n",
    "unique, counts = np.unique(cell_type_labels, return_counts=True)\n",
    "axes[1].bar(unique, counts)\n",
    "axes[1].set_xlabel('Cell Type')\n",
    "axes[1].set_ylabel('Number of Cells')\n",
    "axes[1].set_title('Cell Type Distribution')\n",
    "\n",
    "# Sparsity per cell\n",
    "sparsity_per_cell = (expression_data == 0).mean(axis=1)\n",
    "axes[2].hist(sparsity_per_cell, bins=30, alpha=0.7)\n",
    "axes[2].set_xlabel('Sparsity (fraction of zeros)')\n",
    "axes[2].set_ylabel('Number of Cells')\n",
    "axes[2].set_title('Sparsity Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Dataset\n",
    "\n",
    "Use the scIDiff SingleCellDataset class to handle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata dictionary\n",
    "metadata = {\n",
    "    'cell_type': cell_type_labels,\n",
    "    'batch': np.random.randint(0, 3, len(cell_type_labels)),  # 3 batches\n",
    "    'total_counts': expression_data.sum(axis=1)\n",
    "}\n",
    "\n",
    "# Create dataset\n",
    "dataset = SingleCellDataset(\n",
    "    expression_data=expression_data,\n",
    "    cell_metadata=metadata,\n",
    "    normalize=True  # Apply log1p normalization\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} cells\")\n",
    "print(f\"Dataset statistics: {dataset.get_statistics()}\")\n",
    "print(f\"Cell type categories: {dataset.get_metadata_categories('cell_type')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset access\n",
    "sample = dataset[0]\n",
    "print(\"Sample keys:\", list(sample.keys()))\n",
    "print(\"Expression shape:\", sample['expression'].shape)\n",
    "print(\"Cell type:\", sample['cell_type'].item())\n",
    "print(\"Batch:\", sample['batch'].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Initialization\n",
    "\n",
    "Now let's initialize the scIDiff model with appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_config = {\n",
    "    'gene_dim': dataset.n_genes,          # Number of genes\n",
    "    'hidden_dim': 512,                    # Hidden dimension\n",
    "    'num_layers': 6,                      # Number of layers\n",
    "    'num_timesteps': 1000,                # Diffusion timesteps\n",
    "    'conditioning_dim': 128,              # Conditioning dimension\n",
    "    'dropout': 0.1,                       # Dropout rate\n",
    "    'use_attention': True,                # Use attention layers\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = ScIDiffModel(**model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading\n",
    "\n",
    "Prepare the data for training using PyTorch DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {train_loader.batch_size}\")\n",
    "\n",
    "# Test data loader\n",
    "batch = next(iter(train_loader))\n",
    "print(\"\\nBatch keys:\", list(batch.keys()))\n",
    "print(\"Batch expression shape:\", batch['expression'].shape)\n",
    "print(\"Batch cell_type shape:\", batch['cell_type'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train the scIDiff model using the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = ScIDiffTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    log_dir='./logs',\n",
    "    checkpoint_dir='./checkpoints',\n",
    "    use_wandb=False,  # Set to True if you want to use Weights & Biases\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully\")\n",
    "print(trainer.get_model_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (reduced epochs for demo)\n",
    "trainer.train(\n",
    "    num_epochs=5,  # Use more epochs for real training\n",
    "    save_every=2,\n",
    "    validate_every=1,\n",
    "    log_every=1\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Generation\n",
    "\n",
    "Generate new single-cell expression profiles using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unconditional samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_samples = model.sample(batch_size=100)\n",
    "    generated_samples = generated_samples.cpu().numpy()\n",
    "\n",
    "print(f\"Generated samples shape: {generated_samples.shape}\")\n",
    "print(f\"Generated samples sparsity: {(generated_samples == 0).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare real vs generated data\n",
    "real_data = dataset.expression_data.numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Expression distributions\n",
    "axes[0, 0].hist(real_data.flatten(), bins=50, alpha=0.7, label='Real', density=True)\n",
    "axes[0, 0].hist(generated_samples.flatten(), bins=50, alpha=0.7, label='Generated', density=True)\n",
    "axes[0, 0].set_xlabel('Log Expression')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('Expression Distribution Comparison')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Mean expression per gene\n",
    "real_mean = real_data.mean(axis=0)\n",
    "gen_mean = generated_samples.mean(axis=0)\n",
    "axes[0, 1].scatter(real_mean, gen_mean, alpha=0.5)\n",
    "axes[0, 1].plot([0, real_mean.max()], [0, real_mean.max()], 'r--')\n",
    "axes[0, 1].set_xlabel('Real Mean Expression')\n",
    "axes[0, 1].set_ylabel('Generated Mean Expression')\n",
    "axes[0, 1].set_title('Mean Expression Correlation')\n",
    "\n",
    "# Variance comparison\n",
    "real_var = real_data.var(axis=0)\n",
    "gen_var = generated_samples.var(axis=0)\n",
    "axes[1, 0].scatter(real_var, gen_var, alpha=0.5)\n",
    "axes[1, 0].plot([0, real_var.max()], [0, real_var.max()], 'r--')\n",
    "axes[1, 0].set_xlabel('Real Variance')\n",
    "axes[1, 0].set_ylabel('Generated Variance')\n",
    "axes[1, 0].set_title('Variance Correlation')\n",
    "\n",
    "# Sparsity comparison\n",
    "real_sparsity = (real_data == 0).mean(axis=0)\n",
    "gen_sparsity = (generated_samples == 0).mean(axis=0)\n",
    "axes[1, 1].scatter(real_sparsity, gen_sparsity, alpha=0.5)\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'r--')\n",
    "axes[1, 1].set_xlabel('Real Sparsity')\n",
    "axes[1, 1].set_ylabel('Generated Sparsity')\n",
    "axes[1, 1].set_title('Sparsity Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conditional Generation\n",
    "\n",
    "Generate samples conditioned on specific cell types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples for each cell type\n",
    "cell_type_samples = {}\n",
    "\n",
    "for cell_type in range(5):  # 5 cell types\n",
    "    conditioning = {\n",
    "        'cell_type': torch.tensor([cell_type] * 50, device=device)\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        samples = model.sample(batch_size=50, conditioning=conditioning)\n",
    "        cell_type_samples[cell_type] = samples.cpu().numpy()\n",
    "\n",
    "print(\"Generated conditional samples for all cell types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inverse Design\n",
    "\n",
    "Now let's demonstrate the inverse design capability - generating cells with specific gene expression targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gene name to index mapping (for demo purposes)\n",
    "gene_names = dataset.get_gene_names()\n",
    "gene_to_idx = {name: idx for idx, name in enumerate(gene_names)}\n",
    "\n",
    "# Add some \"marker\" genes\n",
    "marker_genes = ['Gene_0', 'Gene_1', 'Gene_2', 'Gene_10', 'Gene_20']\n",
    "suppressed_genes = ['Gene_100', 'Gene_200', 'Gene_300']\n",
    "\n",
    "print(f\"Marker genes: {marker_genes}\")\n",
    "print(f\"Suppressed genes: {suppressed_genes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup inverse design\n",
    "objective_function = GeneExpressionObjective(gene_to_idx)\n",
    "designer = InverseDesigner(\n",
    "    model=model,\n",
    "    objective_functions=[objective_function],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Define target phenotype\n",
    "target_phenotype = PhenotypeTarget(\n",
    "    gene_targets={\n",
    "        'Gene_0': 5.0,   # High expression\n",
    "        'Gene_1': 4.0,   # High expression\n",
    "        'Gene_2': 3.0,   # Medium-high expression\n",
    "    },\n",
    "    marker_genes=marker_genes,\n",
    "    suppressed_genes=suppressed_genes,\n",
    "    cell_type='custom'\n",
    ")\n",
    "\n",
    "print(\"Target phenotype defined\")\n",
    "print(f\"Gene targets: {target_phenotype.gene_targets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inverse design\n",
    "designed_cells = designer.design(\n",
    "    target=target_phenotype,\n",
    "    num_samples=32,\n",
    "    num_optimization_steps=20,  # Reduced for demo\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "designed_cells_np = designed_cells.cpu().numpy()\n",
    "print(f\"Designed cells shape: {designed_cells_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save and Load Model\n",
    "\n",
    "Demonstrate how to save and load the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = 'scidiff_demo_model.pt'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': model_config,\n",
    "    'dataset_stats': dataset.get_statistics()\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "checkpoint = torch.load(model_save_path, map_location=device)\n",
    "\n",
    "# Create new model instance\n",
    "loaded_model = ScIDiffModel(**checkpoint['model_config'])\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Loaded model has {sum(p.numel() for p in loaded_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loaded model\n",
    "with torch.no_grad():\n",
    "    test_samples = loaded_model.sample(batch_size=10)\n",
    "    print(f\"Generated test samples shape: {test_samples.shape}\")\n",
    "    print(\"Model loading and testing successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "In this tutorial, we demonstrated:\n",
    "\n",
    "1. **Data Preparation**: Creating synthetic single-cell data and using SingleCellDataset\n",
    "2. **Model Initialization**: Setting up the scIDiff model\n",
    "3. **Training**: Training the diffusion model on single-cell data\n",
    "4. **Generation**: Generating new single-cell expression profiles\n",
    "5. **Conditional Generation**: Generating samples conditioned on cell types\n",
    "6. **Inverse Design**: Designing cells with specific gene expression targets\n",
    "7. **Model Persistence**: Saving and loading trained models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try with real single-cell datasets (load with scanpy)\n",
    "- Experiment with different model architectures\n",
    "- Implement custom objective functions for inverse design\n",
    "- Explore perturbation prediction capabilities\n",
    "- Scale up training with larger datasets and more epochs\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- scIDiff provides a flexible framework for single-cell generative modeling\n",
    "- The inverse design capability enables targeted cellular state generation\n",
    "- The model can be conditioned on various biological covariates\n",
    "- Proper evaluation is crucial for assessing generation quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

