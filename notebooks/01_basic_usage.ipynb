{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scIDiff Basic Usage Tutorial\n",
    "\n",
    "This notebook demonstrates the basic usage of scIDiff for single-cell gene expression modeling and inverse design.\n",
    "\n",
    "## Overview\n",
    "\n",
    "scIDiff is a deep generative framework that uses diffusion models to:\n",
    "- Generate realistic single-cell expression profiles\n",
    "- Denoise scRNA-seq data\n",
    "- Predict perturbation responses\n",
    "- Perform inverse design of cellular states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import scIDiff components\n",
    "from scIDiff.models import ScIDiffModel\n",
    "from scIDiff.training import ScIDiffTrainer\n",
    "from scIDiff.sampling import InverseDesigner, PhenotypeTarget\n",
    "from scIDiff.sampling.inverse_design import GeneExpressionObjective\n",
    "from scIDiff.data import SingleCellDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "First, let's create some synthetic single-cell data for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic single-cell data\n",
    "def create_synthetic_data(n_cells=1000, n_genes=2000, n_cell_types=5):\n",
    "    \"\"\"\n",
    "    Create synthetic single-cell RNA-seq data\n",
    "    \"\"\"\n",
    "    # Generate cell type labels\n",
    "    cell_types = np.random.randint(0, n_cell_types, n_cells)\n",
    "    \n",
    "    # Generate gene expression data with cell type specific patterns\n",
    "    expression_data = []\n",
    "    \n",
    "    for cell_type in range(n_cell_types):\n",
    "        n_cells_type = np.sum(cell_types == cell_type)\n",
    "        \n",
    "        # Create cell type specific expression pattern\n",
    "        base_expression = np.random.lognormal(0, 1, (n_cells_type, n_genes))\n",
    "        \n",
    "        # Add cell type specific markers\n",
    "        marker_genes = np.random.choice(n_genes, 50, replace=False)\n",
    "        base_expression[:, marker_genes] *= (cell_type + 1) * 2\n",
    "        \n",
    "        # Add sparsity (many genes are not expressed)\n",
    "        sparsity_mask = np.random.random((n_cells_type, n_genes)) < 0.7\n",
    "        base_expression[sparsity_mask] = 0\n",
    "        \n",
    "        expression_data.append(base_expression)\n",
    "    \n",
    "    # Combine all cell types\n",
    "    expression_matrix = np.vstack(expression_data)\n",
    "    \n",
    "    # Log-normalize\n",
    "    expression_matrix = np.log1p(expression_matrix)\n",
    "    \n",
    "    return expression_matrix, cell_types\n",
    "\n",
    "# Generate synthetic data\n",
    "expression_data, cell_type_labels = create_synthetic_data()\n",
    "\n",
    "print(f\"Expression data shape: {expression_data.shape}\")\n",
    "print(f\"Cell types: {np.unique(cell_type_labels)}\")\n",
    "print(f\"Sparsity: {(expression_data == 0).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Expression distribution\n",
    "axes[0].hist(expression_data.flatten(), bins=50, alpha=0.7)\n",
    "axes[0].set_xlabel('Log Expression')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Expression Distribution')\n",
    "\n",
    "# Cell type distribution\n",
    "unique, counts = np.unique(cell_type_labels, return_counts=True)\n",
    "axes[1].bar(unique, counts)\n",
    "axes[1].set_xlabel('Cell Type')\n",
    "axes[1].set_ylabel('Number of Cells')\n",
    "axes[1].set_title('Cell Type Distribution')\n",
    "\n",
    "# Sparsity per cell\n",
    "sparsity_per_cell = (expression_data == 0).mean(axis=1)\n",
    "axes[2].hist(sparsity_per_cell, bins=30, alpha=0.7)\n",
    "axes[2].set_xlabel('Sparsity (fraction of zeros)')\n",
    "axes[2].set_ylabel('Number of Cells')\n",
    "axes[2].set_title('Sparsity Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Initialization\n",
    "\n",
    "Now let's initialize the scIDiff model with appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_config = {\n",
    "    'gene_dim': expression_data.shape[1],  # Number of genes\n",
    "    'hidden_dim': 512,                     # Hidden dimension\n",
    "    'num_layers': 6,                       # Number of layers\n",
    "    'num_timesteps': 1000,                 # Diffusion timesteps\n",
    "    'conditioning_dim': 128,               # Conditioning dimension\n",
    "    'dropout': 0.1,                        # Dropout rate\n",
    "    'use_attention': True,                 # Use attention layers\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = ScIDiffModel(**model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "Prepare the data for training using PyTorch DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert to tensors\n",
    "expression_tensor = torch.FloatTensor(expression_data)\n",
    "cell_type_tensor = torch.LongTensor(cell_type_labels)\n",
    "\n",
    "# Create dataset\n",
    "class SimpleScDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, expression, cell_types):\n",
    "        self.expression = expression\n",
    "        self.cell_types = cell_types\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.expression)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'expression': self.expression[idx],\n",
    "            'conditioning': {\n",
    "                'cell_type': self.cell_types[idx]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(expression_tensor))\n",
    "val_size = len(expression_tensor) - train_size\n",
    "\n",
    "train_expression = expression_tensor[:train_size]\n",
    "train_cell_types = cell_type_tensor[:train_size]\n",
    "val_expression = expression_tensor[train_size:]\n",
    "val_cell_types = cell_type_tensor[train_size:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SimpleScDataset(train_expression, train_cell_types)\n",
    "val_dataset = SimpleScDataset(val_expression, val_cell_types)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {train_loader.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Train the scIDiff model using the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = ScIDiffTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    log_dir='./logs',\n",
    "    checkpoint_dir='./checkpoints',\n",
    "    use_wandb=False,  # Set to True if you want to use Weights & Biases\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully\")\n",
    "print(trainer.get_model_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (reduced epochs for demo)\n",
    "trainer.train(\n",
    "    num_epochs=10,  # Use more epochs for real training\n",
    "    save_every=5,\n",
    "    validate_every=1,\n",
    "    log_every=1\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Generation\n",
    "\n",
    "Generate new single-cell expression profiles using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unconditional samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_samples = model.sample(batch_size=100)\n",
    "    generated_samples = generated_samples.cpu().numpy()\n",
    "\n",
    "print(f\"Generated samples shape: {generated_samples.shape}\")\n",
    "print(f\"Generated samples sparsity: {(generated_samples == 0).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare real vs generated data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Expression distributions\n",
    "axes[0, 0].hist(expression_data.flatten(), bins=50, alpha=0.7, label='Real', density=True)\n",
    "axes[0, 0].hist(generated_samples.flatten(), bins=50, alpha=0.7, label='Generated', density=True)\n",
    "axes[0, 0].set_xlabel('Log Expression')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('Expression Distribution Comparison')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Mean expression per gene\n",
    "real_mean = expression_data.mean(axis=0)\n",
    "gen_mean = generated_samples.mean(axis=0)\n",
    "axes[0, 1].scatter(real_mean, gen_mean, alpha=0.5)\n",
    "axes[0, 1].plot([0, real_mean.max()], [0, real_mean.max()], 'r--')\n",
    "axes[0, 1].set_xlabel('Real Mean Expression')\n",
    "axes[0, 1].set_ylabel('Generated Mean Expression')\n",
    "axes[0, 1].set_title('Mean Expression Correlation')\n",
    "\n",
    "# Variance comparison\n",
    "real_var = expression_data.var(axis=0)\n",
    "gen_var = generated_samples.var(axis=0)\n",
    "axes[1, 0].scatter(real_var, gen_var, alpha=0.5)\n",
    "axes[1, 0].plot([0, real_var.max()], [0, real_var.max()], 'r--')\n",
    "axes[1, 0].set_xlabel('Real Variance')\n",
    "axes[1, 0].set_ylabel('Generated Variance')\n",
    "axes[1, 0].set_title('Variance Correlation')\n",
    "\n",
    "# Sparsity comparison\n",
    "real_sparsity = (expression_data == 0).mean(axis=0)\n",
    "gen_sparsity = (generated_samples == 0).mean(axis=0)\n",
    "axes[1, 1].scatter(real_sparsity, gen_sparsity, alpha=0.5)\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'r--')\n",
    "axes[1, 1].set_xlabel('Real Sparsity')\n",
    "axes[1, 1].set_ylabel('Generated Sparsity')\n",
    "axes[1, 1].set_title('Sparsity Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conditional Generation\n",
    "\n",
    "Generate samples conditioned on specific cell types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples for each cell type\n",
    "cell_type_samples = {}\n",
    "\n",
    "for cell_type in range(5):  # 5 cell types\n",
    "    conditioning = {\n",
    "        'cell_type': torch.tensor([cell_type] * 50, device=device)\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        samples = model.sample(batch_size=50, conditioning=conditioning)\n",
    "        cell_type_samples[cell_type] = samples.cpu().numpy()\n",
    "\n",
    "print(\"Generated conditional samples for all cell types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cell type specific generation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Mean expression per cell type\n",
    "cell_type_means = []\n",
    "for ct in range(5):\n",
    "    real_ct_data = expression_data[cell_type_labels == ct]\n",
    "    gen_ct_data = cell_type_samples[ct]\n",
    "    \n",
    "    real_mean = real_ct_data.mean()\n",
    "    gen_mean = gen_ct_data.mean()\n",
    "    \n",
    "    cell_type_means.append([real_mean, gen_mean])\n",
    "\n",
    "cell_type_means = np.array(cell_type_means)\n",
    "\n",
    "x = np.arange(5)\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, cell_type_means[:, 0], width, label='Real', alpha=0.7)\n",
    "axes[0].bar(x + width/2, cell_type_means[:, 1], width, label='Generated', alpha=0.7)\n",
    "axes[0].set_xlabel('Cell Type')\n",
    "axes[0].set_ylabel('Mean Expression')\n",
    "axes[0].set_title('Mean Expression by Cell Type')\n",
    "axes[0].legend()\n",
    "axes[0].set_xticks(x)\n",
    "\n",
    "# Expression correlation between real and generated for each cell type\n",
    "correlations = []\n",
    "for ct in range(5):\n",
    "    real_ct_data = expression_data[cell_type_labels == ct]\n",
    "    gen_ct_data = cell_type_samples[ct]\n",
    "    \n",
    "    real_mean_genes = real_ct_data.mean(axis=0)\n",
    "    gen_mean_genes = gen_ct_data.mean(axis=0)\n",
    "    \n",
    "    corr = np.corrcoef(real_mean_genes, gen_mean_genes)[0, 1]\n",
    "    correlations.append(corr)\n",
    "\n",
    "axes[1].bar(range(5), correlations, alpha=0.7)\n",
    "axes[1].set_xlabel('Cell Type')\n",
    "axes[1].set_ylabel('Gene Expression Correlation')\n",
    "axes[1].set_title('Real vs Generated Correlation by Cell Type')\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average correlation across cell types: {np.mean(correlations):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inverse Design\n",
    "\n",
    "Now let's demonstrate the inverse design capability - generating cells with specific gene expression targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gene name to index mapping (for demo purposes)\n",
    "gene_names = [f'Gene_{i}' for i in range(expression_data.shape[1])]\n",
    "gene_to_idx = {name: idx for idx, name in enumerate(gene_names)}\n",
    "\n",
    "# Add some \"marker\" genes\n",
    "marker_genes = ['Gene_0', 'Gene_1', 'Gene_2', 'Gene_10', 'Gene_20']\n",
    "suppressed_genes = ['Gene_100', 'Gene_200', 'Gene_300']\n",
    "\n",
    "print(f\"Marker genes: {marker_genes}\")\n",
    "print(f\"Suppressed genes: {suppressed_genes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup inverse design\n",
    "objective_function = GeneExpressionObjective(gene_to_idx)\n",
    "designer = InverseDesigner(\n",
    "    model=model,\n",
    "    objective_functions=[objective_function],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Define target phenotype\n",
    "target_phenotype = PhenotypeTarget(\n",
    "    gene_targets={\n",
    "        'Gene_0': 5.0,   # High expression\n",
    "        'Gene_1': 4.0,   # High expression\n",
    "        'Gene_2': 3.0,   # Medium-high expression\n",
    "    },\n",
    "    marker_genes=marker_genes,\n",
    "    suppressed_genes=suppressed_genes,\n",
    "    cell_type='custom'\n",
    ")\n",
    "\n",
    "print(\"Target phenotype defined\")\n",
    "print(f\"Gene targets: {target_phenotype.gene_targets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inverse design\n",
    "designed_cells = designer.design(\n",
    "    target=target_phenotype,\n",
    "    num_samples=32,\n",
    "    num_optimization_steps=50,  # Reduced for demo\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "designed_cells_np = designed_cells.cpu().numpy()\n",
    "print(f\"Designed cells shape: {designed_cells_np.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate inverse design results\n",
    "evaluation_metrics = designer.evaluate_design(designed_cells, target_phenotype)\n",
    "\n",
    "print(\"Inverse Design Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize inverse design results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Target gene expression levels\n",
    "target_genes = list(target_phenotype.gene_targets.keys())\n",
    "target_indices = [gene_to_idx[gene] for gene in target_genes]\n",
    "target_values = list(target_phenotype.gene_targets.values())\n",
    "achieved_values = designed_cells_np[:, target_indices].mean(axis=0)\n",
    "\n",
    "x = np.arange(len(target_genes))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, target_values, width, label='Target', alpha=0.7)\n",
    "axes[0, 0].bar(x + width/2, achieved_values, width, label='Achieved', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Target Genes')\n",
    "axes[0, 0].set_ylabel('Expression Level')\n",
    "axes[0, 0].set_title('Target vs Achieved Expression')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(target_genes, rotation=45)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Marker genes expression\n",
    "marker_indices = [gene_to_idx[gene] for gene in marker_genes]\n",
    "marker_expression = designed_cells_np[:, marker_indices]\n",
    "random_expression = np.random.choice(designed_cells_np.flatten(), size=marker_expression.shape)\n",
    "\n",
    "axes[0, 1].boxplot([marker_expression.flatten(), random_expression.flatten()], \n",
    "                   labels=['Marker Genes', 'Random Genes'])\n",
    "axes[0, 1].set_ylabel('Expression Level')\n",
    "axes[0, 1].set_title('Marker Genes vs Random Genes')\n",
    "\n",
    "# Suppressed genes expression\n",
    "suppressed_indices = [gene_to_idx[gene] for gene in suppressed_genes]\n",
    "suppressed_expression = designed_cells_np[:, suppressed_indices]\n",
    "\n",
    "axes[1, 0].boxplot([suppressed_expression.flatten(), random_expression.flatten()], \n",
    "                   labels=['Suppressed Genes', 'Random Genes'])\n",
    "axes[1, 0].set_ylabel('Expression Level')\n",
    "axes[1, 0].set_title('Suppressed Genes vs Random Genes')\n",
    "\n",
    "# Overall expression distribution\n",
    "axes[1, 1].hist(expression_data.flatten(), bins=50, alpha=0.5, label='Original Data', density=True)\n",
    "axes[1, 1].hist(designed_cells_np.flatten(), bins=50, alpha=0.5, label='Designed Cells', density=True)\n",
    "axes[1, 1].set_xlabel('Expression Level')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_title('Expression Distribution Comparison')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Let's evaluate the model's performance using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "def calculate_metrics(real_data, generated_data):\n",
    "    \"\"\"\n",
    "    Calculate various evaluation metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Mean and variance correlation\n",
    "    real_mean = real_data.mean(axis=0)\n",
    "    gen_mean = generated_data.mean(axis=0)\n",
    "    metrics['mean_correlation'] = np.corrcoef(real_mean, gen_mean)[0, 1]\n",
    "    \n",
    "    real_var = real_data.var(axis=0)\n",
    "    gen_var = generated_data.var(axis=0)\n",
    "    metrics['variance_correlation'] = np.corrcoef(real_var, gen_var)[0, 1]\n",
    "    \n",
    "    # Sparsity comparison\n",
    "    real_sparsity = (real_data == 0).mean()\n",
    "    gen_sparsity = (generated_data == 0).mean()\n",
    "    metrics['sparsity_difference'] = abs(real_sparsity - gen_sparsity)\n",
    "    \n",
    "    # Distribution similarity (using Wasserstein distance)\n",
    "    from scipy.stats import wasserstein_distance\n",
    "    metrics['wasserstein_distance'] = wasserstein_distance(\n",
    "        real_data.flatten(), generated_data.flatten()\n",
    "    )\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate unconditional generation\n",
    "unconditional_metrics = calculate_metrics(expression_data, generated_samples)\n",
    "\n",
    "print(\"Unconditional Generation Metrics:\")\n",
    "for metric, value in unconditional_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save and Load Model\n",
    "\n",
    "Demonstrate how to save and load the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = 'scidiff_demo_model.pt'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': model_config,\n",
    "    'training_metrics': unconditional_metrics\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "checkpoint = torch.load(model_save_path, map_location=device)\n",
    "\n",
    "# Create new model instance\n",
    "loaded_model = ScIDiffModel(**checkpoint['model_config'])\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Loaded model has {sum(p.numel() for p in loaded_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loaded model\n",
    "with torch.no_grad():\n",
    "    test_samples = loaded_model.sample(batch_size=10)\n",
    "    print(f\"Generated test samples shape: {test_samples.shape}\")\n",
    "    print(\"Model loading and testing successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "In this tutorial, we demonstrated:\n",
    "\n",
    "1. **Data Preparation**: Creating synthetic single-cell data\n",
    "2. **Model Initialization**: Setting up the scIDiff model\n",
    "3. **Training**: Training the diffusion model on single-cell data\n",
    "4. **Generation**: Generating new single-cell expression profiles\n",
    "5. **Conditional Generation**: Generating samples conditioned on cell types\n",
    "6. **Inverse Design**: Designing cells with specific gene expression targets\n",
    "7. **Evaluation**: Assessing model performance with various metrics\n",
    "8. **Model Persistence**: Saving and loading trained models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try with real single-cell datasets\n",
    "- Experiment with different model architectures\n",
    "- Implement custom objective functions for inverse design\n",
    "- Explore perturbation prediction capabilities\n",
    "- Scale up training with larger datasets and more epochs\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- scIDiff provides a flexible framework for single-cell generative modeling\n",
    "- The inverse design capability enables targeted cellular state generation\n",
    "- The model can be conditioned on various biological covariates\n",
    "- Proper evaluation is crucial for assessing generation quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

